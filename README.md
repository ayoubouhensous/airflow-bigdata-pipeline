# Big Data Pipeline with Apache Airflow

This project implements a **complete Big Data pipeline orchestrated with Apache Airflow**, using a **lakehouse-style architecture**.  
The pipeline handles data ingestion, validation, transformation, and loading into a curated layer ready for **analytics and machine learning**.

---

## ğŸ§© Pipeline Overview

The DAG `bigdata_pipeline_complete` executes the following tasks:

1ï¸âƒ£ **Data Ingestion**
- Generates a raw dataset (`sales.csv`)
- Stores raw data in the `raw` layer  
ğŸ“‚ `./data/raw`

2ï¸âƒ£ **Data Validation**
- Checks if the raw dataset exists
- Prevents pipeline execution if data is missing

3ï¸âƒ£ **Data Transformation**
- Cleans and processes raw data
- Outputs processed data to the `processed` layer  
ğŸ“‚ `./data/processed`

4ï¸âƒ£ **Load to Lakehouse**
- Moves processed data to the curated layer
- Prepares data for analytics and ML workloads  
ğŸ“‚ `./data/curated`

5ï¸âƒ£ **Analytics / ML Ready**
- Confirms that data is ready for BI / ML

### ğŸ”— Pipeline Workflow

```

Ingest â†’ Validate â†’ Transform â†’ Load (Lakehouse) â†’ Analytics

````

- **Schedule:** Daily
- **Executor:** LocalExecutor
- **Database:** PostgreSQL

---

## ğŸ³ Docker Compose Architecture

The project uses **Docker Compose** to deploy and manage Airflow services:

### ğŸ“¦ Services Included

- **PostgreSQL**
  - Stores Airflow metadata
- **Airflow Webserver**
  - Provides Airflow Web UI
- **Airflow Scheduler**
  - Executes DAG tasks

### âš™ï¸ Volumes Mounted

- `./dags` â†’ Airflow DAGs
- `./data` â†’ Raw, processed, and curated data

---

## ğŸ How to Run the Pipeline (First Time Setup)

### 1ï¸âƒ£ Initialize the Airflow metadata database *(run once)*

```bash
docker-compose run airflow-webserver airflow db init
````

---

### 2ï¸âƒ£ Create the Airflow admin user *(run once)*

```bash
docker-compose run airflow-webserver airflow users create \
  --username airflow \
  --password airflow \
  --firstname Airflow \
  --lastname Admin \
  --role Admin \
  --email admin@airflow.local
```

---

### 3ï¸âƒ£ Start Airflow services

```bash
docker-compose up -d
```

---

### 4ï¸âƒ£ Access the Airflow Web UI

Open your browser:

```
http://localhost:8080
```

**Login credentials:**

* **Username:** airflow
* **Password:** airflow

---

### 5ï¸âƒ£ Trigger the Pipeline DAG

* In the Airflow Web UI, go to **DAGs**
* Locate `bigdata_pipeline_complete`
* **Turn on** the DAG
* Either **wait for the daily schedule** or **trigger manually**

---

### 6ï¸âƒ£ Verify Data Layers

After the DAG runs successfully:

* **Raw layer:** `./data/raw/sales.csv`
* **Processed layer:** `./data/processed/sales_clean.csv`
* **Curated layer:** `./data/curated/sales_curated.csv`

---

### 7ï¸âƒ£ Ready for Analytics / ML

* Data is clean and ready for **Business Intelligence** or **Machine Learning tasks**.
* Connect your BI tools or ML scripts directly to the curated layer.

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ bigdata_pipeline_complete.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ curated/
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

## âœ… Key Features

* End-to-end ETL pipeline
* Airflow orchestration
* Lakehouse-style data layers
* Dockerized deployment
* Ready for BI & Machine Learning

```
